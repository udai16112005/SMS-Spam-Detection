{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":75339982,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> # **Facial Landmark Detection project**","metadata":{}},{"cell_type":"markdown","source":"# **Dataset link:-**\nhttps://www.kaggle.com/code/janvichokshi/facial-landmark-detection-tensorflow/input","metadata":{}},{"cell_type":"markdown","source":"**Project Outline**:\n\nFacial Keypoint Detection from Video Frames\n1. Import Required Libraries\nImport essential libraries for:\n\nData Handling: NumPy, Pandas\n\nVisualization: Matplotlib\n\nImage Processing: OpenCV\n\nDeep Learning: TensorFlow / Keras\n\n2. Load and Prepare Dataset\nLoad metadata from a CSV file containing video IDs and facial keypoint annotations.\n\nUse glob to locate and list all .npz video files.\n\nMap video IDs to file paths and filter out entries with missing video data.\n\n3. Data Preprocessing\nExtract the first frame from each video and resize it to a uniform resolution (e.g., 90x90 pixels).\n\nExtract corresponding 2D facial keypoints and scale them to match the resized images.\n\nConvert both image and keypoint data into NumPy arrays suitable for model training.\n\n4. Data Visualization\nDisplay sample frames with facial landmarks overlaid to verify keypoint alignment.\n\nCompare image-keypoint samples before and after normalization for sanity check.\n\n5. Model Architecture Design\nBuild a Convolutional Neural Network (CNN) using the Keras Sequential API.\n\nArchitecture includes:\n\nMultiple Conv2D layers with ReLU activation\n\nBatchNormalization and Dropout for regularization\n\nFinal output layer predicts normalized facial keypoints\n\n6. Model Training\nCompile the model using:\n\nLoss Function: Mean Squared Error (MSE)\n\nOptimizer: Adam\n\nTrain the model on the preprocessed images and keypoints over several epochs with appropriate batch size and validation.\n\n7. Model Saving and Reusability\nSave the trained model to disk for future inference.\n\nImplement functionality to load the saved model for evaluation or prediction.\n\n8. Model Evaluation and Testing\nCreate a test set by sampling and processing new frames.\n\nUse the trained model to predict facial landmarks on test images.\n\nOverlay predicted keypoints on images for visual assessment of model accuracy.\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Importing libraries**\n\n***Description: Import essential libraries for data handling, visualization, and image processing***.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport glob\n# from sklearn import cluster\nimport cv2","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2025-06-27T06:15:26.376439Z","iopub.execute_input":"2025-06-27T06:15:26.376747Z","iopub.status.idle":"2025-06-27T06:15:27.332404Z","shell.execute_reply.started":"2025-06-27T06:15:26.37672Z","shell.execute_reply":"2025-06-27T06:15:27.331285Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# *Load dataset*\n**Description: Load CSV file into a DataFrame and display the first few rows**.","metadata":{}},{"cell_type":"code","source":"videoDF = pd.read_csv('../input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full.csv')\nvideoDF.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Dataset summary**\n**Description: Print the total number of videos and unique individuals in the dataset.**","metadata":{}},{"cell_type":"code","source":"print('Number of Videos is %d' %(videoDF.shape[0]))\nprint('Number of Unique Individuals is %d' %(len(videoDF['personName'].unique())))","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:15:37.310727Z","iopub.status.busy":"2025-06-26T13:15:37.310364Z","iopub.status.idle":"2025-06-26T13:15:37.318944Z","shell.execute_reply":"2025-06-26T13:15:37.318167Z","shell.execute_reply.started":"2025-06-26T13:15:37.310674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Map video IDs to file paths**\n**Description: Collect .npz file paths, map them to video IDs, and filter the dataset to only include videos with available files. Prints updated video and individual counts.**","metadata":{}},{"cell_type":"code","source":"# create a dictionary that maps videoIDs to full file paths\nnpzFilesFullPath = glob.glob('../input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full_1/youtube_faces_with_keypoints_full_1/*.npz')\nnpzFilesFullPath=np.append(npzFilesFullPath,glob.glob('../input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full_2/youtube_faces_with_keypoints_full_2/*.npz'))\nnpzFilesFullPath=np.append(npzFilesFullPath,glob.glob('../input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full_3/youtube_faces_with_keypoints_full_3/*.npz'))\nnpzFilesFullPath=np.append(npzFilesFullPath,glob.glob('../input/youtube-faces-with-facial-keypoints/youtube_faces_with_keypoints_full_4/youtube_faces_with_keypoints_full_4/*.npz'))\n\nprint(npzFilesFullPath[0])\nvideoIDs = [x.split('/')[-1].split('.')[0] for x in npzFilesFullPath]\nfullPaths = {}\nfor videoID, fullPath in zip(videoIDs, npzFilesFullPath):\n    fullPaths[videoID] = fullPath\n\nvideoDF = videoDF.loc[videoDF.loc[:,'videoID'].isin(fullPaths.keys()),:].reset_index(drop=True)\nprint('Number of Videos is %d' %(videoDF.shape[0]))\nprint('Number of Unique Individuals is %d' %(len(videoDF['personName'].unique())))","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:15:37.32244Z","iopub.status.busy":"2025-06-26T13:15:37.322189Z","iopub.status.idle":"2025-06-26T13:15:37.42379Z","shell.execute_reply":"2025-06-26T13:15:37.422798Z","shell.execute_reply.started":"2025-06-26T13:15:37.322415Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Preview filtered dataset**\n**Description: Display the first few rows of the dataset after filtering based on available video files.**","metadata":{}},{"cell_type":"code","source":"videoDF.head()","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:15:37.426785Z","iopub.status.busy":"2025-06-26T13:15:37.426379Z","iopub.status.idle":"2025-06-26T13:15:37.440242Z","shell.execute_reply":"2025-06-26T13:15:37.439473Z","shell.execute_reply.started":"2025-06-26T13:15:37.426743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Display sample video frames with 2D keypoints**\n**Description: Randomly select a few videos and display selected frames with 2D facial keypoints overlaid for visualization.**","metadata":{}},{"cell_type":"code","source":"# show several frames from each video and overlay 2D keypoints\nnp.random.seed(0)\nnumVideos = 4\nframesToShowFromVideo = np.array([0.1,0.3,0.6,0.9])\nnumFramesPerVideo = len(framesToShowFromVideo)\n\n# select a random subset of 'numVideos' from the available videos\nrandVideoIDs = videoDF.loc[np.random.choice(videoDF.index,size=numVideos,replace=False),'videoID']\n# print(listOfAllConnectedPoints.shape)\nfig, axArray = plt.subplots(nrows=numVideos,ncols=numFramesPerVideo,figsize=(14,18))\nfor i, videoID in enumerate(randVideoIDs):\n    # load video\n    videoFile = np.load(fullPaths[videoID])\n    colorImages = videoFile['colorImages']\n    boundingBox = videoFile['boundingBox']\n    landmarks2D = videoFile['landmarks2D']\n\n    selectedFrames = (framesToShowFromVideo*(colorImages.shape[3]-1)).astype(int)\n    for j, frameInd in enumerate(selectedFrames):\n        axArray[i][j].imshow(colorImages[:,:,:,frameInd])\n        axArray[i][j].scatter(x=landmarks2D[:,0,frameInd],y=landmarks2D[:,1,frameInd],s=5,c='b')\n        axArray[i][j].set_title('\"%s\" (t=%d)' %(videoID,frameInd), fontsize=12)\n        axArray[i][j].set_axis_off()","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:15:37.442414Z","iopub.status.busy":"2025-06-26T13:15:37.442059Z","iopub.status.idle":"2025-06-26T13:15:39.07608Z","shell.execute_reply":"2025-06-26T13:15:39.075094Z","shell.execute_reply.started":"2025-06-26T13:15:37.442376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preparing Dataset","metadata":{}},{"cell_type":"markdown","source":"# **Count video entries**\n**Description: Display the total number of video entries in the DataFrame**","metadata":{}},{"cell_type":"code","source":"len(videoDF['videoID'])","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:15:39.077607Z","iopub.status.busy":"2025-06-26T13:15:39.077331Z","iopub.status.idle":"2025-06-26T13:15:39.08297Z","shell.execute_reply":"2025-06-26T13:15:39.082178Z","shell.execute_reply.started":"2025-06-26T13:15:39.07758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Extract first frame per video**\n**Description: Extract and resize the first frame of each video to a fixed size, normalizing pixel values. Tracks progress during processing.**","metadata":{}},{"cell_type":"code","source":"#selecting first frame of each person\nimages=[]\nimg_size=90\nfor i, videoID in enumerate(videoDF['videoID']):\n    if (i%500)==0:\n        print(i*10, ' images saved')\n    videoFile = np.load(fullPaths[videoID])\n    colorImages = videoFile['colorImages']\n    landmarks = videoFile['landmarks2D']\n    images.append(cv2.resize(colorImages[:,:,:,0],(img_size,img_size))/255)","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:15:39.084309Z","iopub.status.busy":"2025-06-26T13:15:39.084019Z","iopub.status.idle":"2025-06-26T13:21:40.90591Z","shell.execute_reply":"2025-06-26T13:21:40.905238Z","shell.execute_reply.started":"2025-06-26T13:15:39.08428Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Convert image list to array**\n**Description: Convert the list of resized video frames into a NumPy array for further processing or model input.**","metadata":{}},{"cell_type":"code","source":"images=np.array(images)","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:21:40.907116Z","iopub.status.busy":"2025-06-26T13:21:40.906905Z","iopub.status.idle":"2025-06-26T13:21:41.039447Z","shell.execute_reply":"2025-06-26T13:21:41.038803Z","shell.execute_reply.started":"2025-06-26T13:21:40.907095Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Extract and scale keypoints**\n**Description: Extract facial keypoints from the first frame of each video and scale them to match the resized image dimensions. Progress is printed periodically.**","metadata":{}},{"cell_type":"code","source":"key_pts=[]\nfor i, videoID in enumerate(videoDF['videoID']):\n    if (i%500)==0:\n        print(i)\n    videoFile = np.load(fullPaths[videoID])\n    org_h,org_w=videoFile['colorImages'][:,:,:,0].shape[:2]\n    scale_h,scale_w=img_size/org_h,img_size/org_w\n    landmarks = videoFile['landmarks2D']\n    keyPts=landmarks[:,:,0]\n    keyPts[:,0]=keyPts[:,0]*scale_w\n    keyPts[:,1]=keyPts[:,1]*scale_h\n    key_pts.append(keyPts)","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:21:41.040806Z","iopub.status.busy":"2025-06-26T13:21:41.040505Z","iopub.status.idle":"2025-06-26T13:25:43.238875Z","shell.execute_reply":"2025-06-26T13:25:43.238121Z","shell.execute_reply.started":"2025-06-26T13:21:41.040778Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Convert keypoints list to array**\n**Description: Convert the list of 2D facial keypoints into a NumPy array for structured analysis or modeling.**","metadata":{}},{"cell_type":"code","source":"keypts=np.array(key_pts)","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:25:43.240301Z","iopub.status.busy":"2025-06-26T13:25:43.240056Z","iopub.status.idle":"2025-06-26T13:25:43.249146Z","shell.execute_reply":"2025-06-26T13:25:43.248383Z","shell.execute_reply.started":"2025-06-26T13:25:43.240278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Visualize sample images with keypoints**\n**Description: Display a few sample images with their corresponding facial keypoints overlaid for visual verification.**","metadata":{}},{"cell_type":"code","source":"fig,ax=plt.subplots(nrows=1,ncols=4,figsize=(15,15))\nfor i in range(4):\n    ax[i].imshow(images[i])\n    ax[i].scatter(keypts[i,:,0],keypts[i,:,1],s=5)","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:25:43.250689Z","iopub.status.busy":"2025-06-26T13:25:43.250388Z","iopub.status.idle":"2025-06-26T13:25:43.860093Z","shell.execute_reply":"2025-06-26T13:25:43.859244Z","shell.execute_reply.started":"2025-06-26T13:25:43.250663Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Prepare keypoints for training**\n**Description: Reshape and normalize keypoints data for model input, scaling coordinates relative to image size.** ","metadata":{}},{"cell_type":"code","source":"# img_size=90\ny_data=keypts.reshape(keypts.shape[0],-1)\ny_train = np.reshape( y_data , ( -1 , 1 , 1 , 136 ))/img_size","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:25:43.861576Z","iopub.status.busy":"2025-06-26T13:25:43.861307Z","iopub.status.idle":"2025-06-26T13:25:43.866558Z","shell.execute_reply":"2025-06-26T13:25:43.86579Z","shell.execute_reply.started":"2025-06-26T13:25:43.861547Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Visualize normalized keypoints on images**\n**Description: Plot sample images with denormalized facial keypoints overlaid to verify keypoint scaling and alignment.**","metadata":{}},{"cell_type":"code","source":"fig,ax=plt.subplots(nrows=1,ncols=4,figsize=(15,15))\nfor i in range(4):\n    ax[i].imshow(images[i])\n    x=np.reshape(y_train[i,:,:,np.arange(0,136,2)],(68))*img_size\n    y=np.reshape(y_train[i,:,:,np.arange(1,136,2)],(68))*img_size\n    ax[i].scatter(x,y,s=5)","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:25:43.868027Z","iopub.status.busy":"2025-06-26T13:25:43.867687Z","iopub.status.idle":"2025-06-26T13:25:44.401393Z","shell.execute_reply":"2025-06-26T13:25:44.400578Z","shell.execute_reply.started":"2025-06-26T13:25:43.867999Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Check keypoints array shape**\n***Description: Display the shape of the prepared keypoints array used for training.***","metadata":{}},{"cell_type":"code","source":"y_train.shape","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:25:44.402653Z","iopub.status.busy":"2025-06-26T13:25:44.40244Z","iopub.status.idle":"2025-06-26T13:25:44.407313Z","shell.execute_reply":"2025-06-26T13:25:44.406547Z","shell.execute_reply.started":"2025-06-26T13:25:44.402632Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model(CNN)","metadata":{}},{"cell_type":"markdown","source":"# **Import TensorFlow and Keras**\n***Description: Import TensorFlow and its Keras API for building and training deep learning models.***","metadata":{}},{"cell_type":"code","source":"import tensorflow\nfrom tensorflow import keras","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:25:44.408679Z","iopub.status.busy":"2025-06-26T13:25:44.408456Z","iopub.status.idle":"2025-06-26T13:25:49.065609Z","shell.execute_reply":"2025-06-26T13:25:49.064893Z","shell.execute_reply.started":"2025-06-26T13:25:44.408656Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Build and compile CNN model**\n***Description: Define a deep convolutional neural network with multiple Conv2D, BatchNormalization, and Dropout layers. Compile it using mean squared error loss and Adam optimizer. Display the model summary.***","metadata":{}},{"cell_type":"code","source":"model_layers=[\n    keras.layers.Conv2D( 256, input_shape=( img_size , img_size , 3 ) , kernel_size=( 5 , 5 ) , strides=1 , activation='relu',name=\"input_layer\"),\n    keras.layers.Conv2D( 256 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' ),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D( 256, kernel_size=( 5 , 5 ) , strides=1 , activation='relu' ),\n    keras.layers.Conv2D( 256, kernel_size=( 5 , 5 ) , strides=1 , activation='relu' ),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D( 200, kernel_size=( 5 , 5 ) , strides=2 , activation='relu'),\n    keras.layers.Conv2D( 200 , kernel_size=( 5 , 5 ) , strides= 1, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D( 200, kernel_size=( 5 , 5 ) , strides=1 , activation='relu'),\n    keras.layers.Conv2D( 200 , kernel_size=( 5 , 5 ) , strides=1 , activation='relu' ),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D( 170, kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n    keras.layers.Conv2D( 170, kernel_size=( 3 , 3 ) , strides=1 , activation='relu' ),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D( 136, kernel_size=( 3 , 3 ) , strides=1 , activation='relu'),\n    keras.layers.Conv2D( 136, kernel_size=( 3 , 3 ) , strides=2 , activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D( 136, kernel_size=( 3 , 3 ) , strides=2 , activation='relu'),\n    keras.layers.Conv2D( 136 , kernel_size=( 3 , 3 ) , strides=1 , activation='sigmoid'),\n\n    \n]\nmodel=keras.Sequential(model_layers)\nmodel.compile( loss= keras.losses.mean_squared_error , optimizer= keras.optimizers.Adam( lr=0.001 ) )\nmodel.summary()","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:25:49.067398Z","iopub.status.busy":"2025-06-26T13:25:49.067065Z","iopub.status.idle":"2025-06-26T13:25:52.40457Z","shell.execute_reply":"2025-06-26T13:25:52.403845Z","shell.execute_reply.started":"2025-06-26T13:25:49.067361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Train model**\n***Description: Train the model on image and keypoint data for 3 epochs with a batch size of 32.***","metadata":{}},{"cell_type":"code","source":"train=model.fit(images,y_train,epochs=3,batch_size=32)","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:25:52.406512Z","iopub.status.busy":"2025-06-26T13:25:52.406163Z","iopub.status.idle":"2025-06-26T13:26:50.557597Z","shell.execute_reply":"2025-06-26T13:26:50.556812Z","shell.execute_reply.started":"2025-06-26T13:25:52.406475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Load pre-trained model**\n***Description: Load a saved Keras model from the file model.pb for further use or evaluation***","metadata":{}},{"cell_type":"code","source":"m=keras.models.load_model('model.pb')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preparing test dataset","metadata":{}},{"cell_type":"markdown","source":"# **Prepare test images and bounding boxes**\n***Description: Extract and resize the last frame from the first 30 videos, scale bounding boxes accordingly, and store them for testing.***","metadata":{}},{"cell_type":"code","source":"test_images=[]\ntest_bbox=[]\nfor i, videoID in enumerate(videoDF['videoID']):\n    if i==30:\n        break\n    videoFile = np.load(fullPaths[videoID])\n    colorImages = videoFile['colorImages']\n    bbox=videoFile['boundingBox']\n    org_h,org_w=colorImages[:,:,:,0].shape[:2]\n    scale_h,scale_w=img_size/org_h,img_size/org_w\n    \n    box=bbox[:,:,-1]\n    box[:,0]=box[:,0]*scale_w\n    box[:,1]=box[:,1]*scale_h\n    test_bbox.append(box)\n    test_images.append(cv2.resize(colorImages[:,:,:,-1],(img_size,img_size))/255)","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:27:34.010545Z","iopub.status.busy":"2025-06-26T13:27:34.010205Z","iopub.status.idle":"2025-06-26T13:27:37.641728Z","shell.execute_reply":"2025-06-26T13:27:37.641021Z","shell.execute_reply.started":"2025-06-26T13:27:34.010511Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# **Convert test bounding boxes to array**\n***Description: Convert the list of scaled bounding boxes into a NumPy array and display its shape.***\n","metadata":{}},{"cell_type":"code","source":"test_bbox=np.array(test_bbox)\ntest_bbox.shape","metadata":{"execution":{"iopub.execute_input":"2025-06-26T13:27:43.049909Z","iopub.status.busy":"2025-06-26T13:27:43.049573Z","iopub.status.idle":"2025-06-26T13:27:43.055728Z","shell.execute_reply":"2025-06-26T13:27:43.054797Z","shell.execute_reply.started":"2025-06-26T13:27:43.04988Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Visualize test images with bounding boxes**\n***Description: Display sample test images with their corresponding bounding boxes overlaid for verification.***","metadata":{}},{"cell_type":"code","source":"test_images=np.array(test_images)\ntest_keypts=np.array(test_keyPts)\nfig,ax=plt.subplots(nrows=1,ncols=4,figsize=(15,15))\nfor i in range(4):\n    ax[i].imshow(test_images[i])\n    ax[i].scatter(test_bbox[i,:,0],test_bbox[i,:,1],s=5)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Visualize model predictions on test images**\n***Description: Predict facial keypoints on test images and plot the results overlaid on the images for visual evaluation.***","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(6,5,figsize=(20,20))\n\nfor i in range(1,30):\n    r=i//5\n    c=i%5\n    sample_image = test_images[i]\n    pred = m.predict( test_images[ i : i +1  ] ) \n    x=np.reshape(pred[:,:,:,np.arange(0,136,2)],(68))*img_size\n    y=np.reshape(pred[:,:,:,np.arange(1,136,2)],(68))*img_size\n    ax[r][c].imshow(sample_image)\n    ax[r][c].scatter( x,y, c='yellow',s=6)","metadata":{"execution":{"iopub.status.busy":"2025-06-26T13:26:50.925824Z","iopub.status.idle":"2025-06-26T13:26:50.926214Z","shell.execute_reply":"2025-06-26T13:26:50.926028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Final Summary**\n\n\nThis project presents a deep learning-based approach for facial landmark detection using TensorFlow and Keras. The objective is to accurately predict 2D facial keypoints from video frames, leveraging a large-scale, video-based facial dataset.\n\nThe workflow begins with loading metadata and corresponding .npz video files. From each video, the first frame is extracted, resized to a uniform resolution, and paired with scaled ground truth facial keypoints. These preprocessed image-keypoint pairs form the input to the model.\n\nA Convolutional Neural Network (CNN) is designed using the Keras Sequential API. The architecture comprises multiple convolutional layers activated with ReLU, along with batch normalization and dropout layers to ensure generalization and reduce overfitting. The final layer outputs normalized coordinates of facial landmarks.\n\nThe pipeline also includes data visualization, allowing for verification of the keypoint annotations and model predictions. The model is trained using the mean squared error loss and the Adam optimizer, and is evaluated by overlaying predicted landmarks on test images for qualitative assessment.\n\nThis solution provides a robust and scalable framework for facial landmark localization, supporting downstream applications such as facial recognition, emotion detection, AR filters, and human-computer interaction.","metadata":{}}]}