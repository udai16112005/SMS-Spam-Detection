{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Object Detection in Tensorflow**","metadata":{}},{"cell_type":"markdown","source":"**Dataset link**\n\n/kaggle/input/data-for-yolo-v3-kernel","metadata":{}},{"cell_type":"markdown","source":" **Project Outline:**\n\n \n1. Introduction\nImplemented the YOLOv3 object detection model using TensorFlow 1.x.\n\nUtilizes pretrained Darknet weights for COCO dataset inference.\n\nDesigned to process static images and visualize bounding box predictions.\n\n2. Modules & Architecture\n🔧 Configuration & Constants\nDefined hyperparameters: _BATCH_NORM_DECAY, _LEAKY_RELU, _ANCHORS, _MODEL_SIZE.\n\n📦 Utility Layers\nbatch_norm, conv2d_fixed_padding, fixed_padding: foundational layers used throughout the model.\n\n🧠 Model Backbone: Darknet-53\nImplemented darknet53_residual_block and darknet53 to extract hierarchical features.\n\nReturns multi-scale feature maps (route1, route2, final_output).\n\n🧱 YOLO Head Construction\nyolo_convolution_block: series of convolutions applied after feature extraction.\n\nyolo_layer: reshapes and decodes feature maps into bounding box predictions.\n\n🔼 Upsampling & Multi-scale Detection\nupsample: increases spatial resolution of feature maps to allow multi-scale fusion.\n\n📤 Postprocessing\nbuild_boxes: transforms center-based box format into [x_min, y_min, x_max, y_max].\n\nnon_max_suppression: filters overlapping and low-confidence detections.\n\n🧩 Model Class\nYolo_v3 class encapsulates the full model architecture, handling:\n\nBackbone construction\n\nYOLO heads for 3 detection scales\n\nPrediction decoding\n\nFinal postprocessing\n\n🖼️ Image I/O & Visualization\nload_images: loads and resizes images to model input shape.\n\nload_class_names: parses COCO label file.\n\ndraw_boxes: visualizes final predictions using bounding boxes and labels.\n\n📥 Weights Loader\nload_weights: parses official Darknet YOLOv3 weights and assigns them to TensorFlow variables.\n\n3. Inference Pipeline\nLoad sample input images (dog.jpg, office.jpg)\n\nInitialize YOLOv3 model with pretrained weights\n\nRun inference in a TensorFlow session\n\nDisplay predictions as static images and/or animated .gif\n\n","metadata":{}},{"cell_type":"markdown","source":"# **import_libraries**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFont\nfrom IPython.display import display\nfrom seaborn import color_palette\nimport cv2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T16:04:16.104498Z","iopub.execute_input":"2025-07-01T16:04:16.104728Z","execution_failed":"2025-07-01T16:04:23.957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" **config_constants**\n \n Defines key hyperparameters and constants used across the model such as batch normalization settings, leaky ReLU slope, anchor box dimensions, and input image size for the YOLO-style object detection model.\n\n","metadata":{"_uuid":"2a30a24e3cc58704b009806e1c6efb1f7ca778f6"}},{"cell_type":"code","source":"_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-05\n_LEAKY_RELU = 0.1\n_ANCHORS = [(10, 13), (16, 30), (33, 23),\n            (30, 61), (62, 45), (59, 119),\n            (116, 90), (156, 198), (373, 326)]\n_MODEL_SIZE = (416, 416)","metadata":{"trusted":true,"_uuid":"8c963b6244cf03abad2defc60e7bb0b5dde7cfda"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"`_MODEL_SIZE` refers to the input size of the model.\n\nLet's look at other parameters step-by-step.","metadata":{"_uuid":"3e88ca967006cf686c53568071030cb968359716"}},{"cell_type":"markdown","source":"**Layers_and_padding_utils**\n\nDefines utility functions for building the model’s convolutional backbone:\n\nbatch_norm: Applies batch normalization with standard parameters based on data format.\n\nfixed_padding: Implements fixed padding for ResNet-style convolutions, independent of input size.\n\nconv2d_fixed_padding: Performs a 2D convolution with optional fixed padding for strided operations.","metadata":{}},{"cell_type":"code","source":"def batch_norm(inputs, training, data_format):\n    \"\"\"Performs a batch normalization using a standard set of parameters.\"\"\"\n    return tf.layers.batch_normalization(\n        inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON,\n        scale=True, training=training)\n\n\ndef fixed_padding(inputs, kernel_size, data_format):\n    \"\"\"ResNet implementation of fixed padding.\n\n    Pads the input along the spatial dimensions independently of input size.\n\n    Args:\n        inputs: Tensor input to be padded.\n        kernel_size: The kernel to be used in the conv2d or max_pool2d.\n        data_format: The input format.\n    Returns:\n        A tensor with the same format as the input.\n    \"\"\"\n    pad_total = kernel_size - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n\n    if data_format == 'channels_first':\n        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                        [pad_beg, pad_end],\n                                        [pad_beg, pad_end]])\n    else:\n        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                        [pad_beg, pad_end], [0, 0]])\n    return padded_inputs\n\n\ndef conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1):\n    \"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n    if strides > 1:\n        inputs = fixed_padding(inputs, kernel_size, data_format)\n\n    return tf.layers.conv2d(\n        inputs=inputs, filters=filters, kernel_size=kernel_size,\n        strides=strides, padding=('SAME' if strides == 1 else 'VALID'),\n        use_bias=False, data_format=data_format)","metadata":{"trusted":true,"_uuid":"80c2ae4a51e7bc745f50997234ed9b738315b19b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Darknet53_model**\n\nImplements the Darknet-53 architecture used as the feature extractor backbone in YOLOv3:\n\ndarknet53_residual_block: Defines a residual unit with 1×1 and 3×3 convolutions.\n\ndarknet53: Builds the full Darknet-53 network, organizing blocks into stages with increasing depth and stride, returning multi-scale feature maps (route1, route2, inputs) used in detection heads.","metadata":{}},{"cell_type":"code","source":"def darknet53_residual_block(inputs, filters, training, data_format,\n                             strides=1):\n    \"\"\"Creates a residual block for Darknet.\"\"\"\n    shortcut = inputs\n\n    inputs = conv2d_fixed_padding(\n        inputs, filters=filters, kernel_size=1, strides=strides,\n        data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs = conv2d_fixed_padding(\n        inputs, filters=2 * filters, kernel_size=3, strides=strides,\n        data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs += shortcut\n\n    return inputs\n\n\ndef darknet53(inputs, training, data_format):\n    \"\"\"Creates Darknet53 model for feature extraction.\"\"\"\n    inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n    inputs = conv2d_fixed_padding(inputs, filters=64, kernel_size=3,\n                                  strides=2, data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs = darknet53_residual_block(inputs, filters=32, training=training,\n                                      data_format=data_format)\n\n    inputs = conv2d_fixed_padding(inputs, filters=128, kernel_size=3,\n                                  strides=2, data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    for _ in range(2):\n        inputs = darknet53_residual_block(inputs, filters=64,\n                                          training=training,\n                                          data_format=data_format)\n\n    inputs = conv2d_fixed_padding(inputs, filters=256, kernel_size=3,\n                                  strides=2, data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    for _ in range(8):\n        inputs = darknet53_residual_block(inputs, filters=128,\n                                          training=training,\n                                          data_format=data_format)\n\n    route1 = inputs\n\n    inputs = conv2d_fixed_padding(inputs, filters=512, kernel_size=3,\n                                  strides=2, data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    for _ in range(8):\n        inputs = darknet53_residual_block(inputs, filters=256,\n                                          training=training,\n                                          data_format=data_format)\n\n    route2 = inputs\n\n    inputs = conv2d_fixed_padding(inputs, filters=1024, kernel_size=3,\n                                  strides=2, data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    for _ in range(4):\n        inputs = darknet53_residual_block(inputs, filters=512,\n                                          training=training,\n                                          data_format=data_format)\n\n    return route1, route2, inputs","metadata":{"trusted":true,"_uuid":"d1fdc7ef8b92f0fdd60ff57911edf48cafe49d83"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Yolo_conv_block**\n\nDefines the convolutional block used in the YOLO detection head:\n\nyolo_convolution_block: Applies a series of alternating 1×1 and 3×3 convolutions with batch normalization and leaky ReLU.\nReturns:\n\nroute: An intermediate feature map used for concatenation in multi-scale detection.\n\ninputs: The final processed feature map used for prediction at the current scale.\n\nThis structure refines the extracted features from Darknet-53 before making bounding box predictions.","metadata":{}},{"cell_type":"code","source":"def yolo_convolution_block(inputs, filters, training, data_format):\n    \"\"\"Creates convolution operations layer used after Darknet.\"\"\"\n    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    route = inputs\n\n    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n                                  data_format=data_format)\n    inputs = batch_norm(inputs, training=training, data_format=data_format)\n    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n\n    return route, inputs","metadata":{"trusted":true,"_uuid":"42c6af6b2a1cc052b69cbb768bfdfeed60e627a0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Yolo_detection_layer**\n\nImplements the final YOLO layer that predicts bounding boxes relative to anchor boxes:\n\nTransforms feature maps into box center coordinates, shapes, confidence scores, and class probabilities.\n\nApplies sigmoid activation to box centers, objectness, and class scores.\n\nCalculates actual box positions using anchor priors and stride scaling.\n\nReturns a tensor of shape [batch, total_boxes, 5 + num_classes] containing decoded predictions.\n\nThis layer is essential for converting raw convolutional outputs into interpretable bounding box predictions.\n\n","metadata":{}},{"cell_type":"code","source":"def yolo_layer(inputs, n_classes, anchors, img_size, data_format):\n    \"\"\"Creates Yolo final detection layer.\n\n    Detects boxes with respect to anchors.\n\n    Args:\n        inputs: Tensor input.\n        n_classes: Number of labels.\n        anchors: A list of anchor sizes.\n        img_size: The input size of the model.\n        data_format: The input format.\n\n    Returns:\n        Tensor output.\n    \"\"\"\n    n_anchors = len(anchors)\n\n    inputs = tf.layers.conv2d(inputs, filters=n_anchors * (5 + n_classes),\n                              kernel_size=1, strides=1, use_bias=True,\n                              data_format=data_format)\n\n    shape = inputs.get_shape().as_list()\n    grid_shape = shape[2:4] if data_format == 'channels_first' else shape[1:3]\n    if data_format == 'channels_first':\n        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n    inputs = tf.reshape(inputs, [-1, n_anchors * grid_shape[0] * grid_shape[1],\n                                 5 + n_classes])\n\n    strides = (img_size[0] // grid_shape[0], img_size[1] // grid_shape[1])\n\n    box_centers, box_shapes, confidence, classes = \\\n        tf.split(inputs, [2, 2, 1, n_classes], axis=-1)\n\n    x = tf.range(grid_shape[0], dtype=tf.float32)\n    y = tf.range(grid_shape[1], dtype=tf.float32)\n    x_offset, y_offset = tf.meshgrid(x, y)\n    x_offset = tf.reshape(x_offset, (-1, 1))\n    y_offset = tf.reshape(y_offset, (-1, 1))\n    x_y_offset = tf.concat([x_offset, y_offset], axis=-1)\n    x_y_offset = tf.tile(x_y_offset, [1, n_anchors])\n    x_y_offset = tf.reshape(x_y_offset, [1, -1, 2])\n    box_centers = tf.nn.sigmoid(box_centers)\n    box_centers = (box_centers + x_y_offset) * strides\n\n    anchors = tf.tile(anchors, [grid_shape[0] * grid_shape[1], 1])\n    box_shapes = tf.exp(box_shapes) * tf.to_float(anchors)\n\n    confidence = tf.nn.sigmoid(confidence)\n\n    classes = tf.nn.sigmoid(classes)\n\n    inputs = tf.concat([box_centers, box_shapes,\n                        confidence, classes], axis=-1)\n\n    return inputs","metadata":{"trusted":true,"_uuid":"9aa8b0d0f0e297787fe809353107ae379d1c7d24"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**upsample_layer**\n\nDefines a utility function for upsampling feature maps using nearest neighbor interpolation:\n\nupsample: Resizes input tensors to a target shape (out_shape) based on data_format (either 'channels_first' or 'channels_last').\n\nUses tf.image.resize_nearest_neighbor for interpolation.\n\nHandles necessary tensor transpositions for compatibility with the data format.\n\nThis function is used in YOLOv3 for merging features across different scales in the detection head.","metadata":{}},{"cell_type":"code","source":"def upsample(inputs, out_shape, data_format):\n    \"\"\"Upsamples to `out_shape` using nearest neighbor interpolation.\"\"\"\n    if data_format == 'channels_first':\n        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n        new_height = out_shape[3]\n        new_width = out_shape[2]\n    else:\n        new_height = out_shape[2]\n        new_width = out_shape[1]\n\n    inputs = tf.image.resize_nearest_neighbor(inputs, (new_height, new_width))\n\n    if data_format == 'channels_first':\n        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n\n    return inputs","metadata":{"trusted":true,"scrolled":true,"_uuid":"31a240155d9f96edf06011a4b51d4cd0a999d24d"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**postprocessing_nms**\n\nHandles bounding box refinement and filtering after YOLO predictions:\n\nbuild_boxes: Converts YOLO’s center-format boxes into top-left and bottom-right corner format ([x_min, y_min, x_max, y_max]), appending confidence and class scores.\n\nnon_max_suppression: Applies Non-Maximum Suppression (NMS) class-wise to remove overlapping boxes based on:\n\nIntersection over Union (IoU)\n\nConfidence threshold\n\nClass labels\nReturns a list of per-class filtered bounding boxes for each image in the batch.\n\nThis step is crucial for producing final, clean detection results.","metadata":{}},{"cell_type":"code","source":"def build_boxes(inputs):\n    \"\"\"Computes top left and bottom right points of the boxes.\"\"\"\n    center_x, center_y, width, height, confidence, classes = \\\n        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)\n\n    top_left_x = center_x - width / 2\n    top_left_y = center_y - height / 2\n    bottom_right_x = center_x + width / 2\n    bottom_right_y = center_y + height / 2\n\n    boxes = tf.concat([top_left_x, top_left_y,\n                       bottom_right_x, bottom_right_y,\n                       confidence, classes], axis=-1)\n\n    return boxes\n\n\ndef non_max_suppression(inputs, n_classes, max_output_size, iou_threshold,\n                        confidence_threshold):\n    \"\"\"Performs non-max suppression separately for each class.\n\n    Args:\n        inputs: Tensor input.\n        n_classes: Number of classes.\n        max_output_size: Max number of boxes to be selected for each class.\n        iou_threshold: Threshold for the IOU.\n        confidence_threshold: Threshold for the confidence score.\n    Returns:\n        A list containing class-to-boxes dictionaries\n            for each sample in the batch.\n    \"\"\"\n    batch = tf.unstack(inputs)\n    boxes_dicts = []\n    for boxes in batch:\n        boxes = tf.boolean_mask(boxes, boxes[:, 4] > confidence_threshold)\n        classes = tf.argmax(boxes[:, 5:], axis=-1)\n        classes = tf.expand_dims(tf.to_float(classes), axis=-1)\n        boxes = tf.concat([boxes[:, :5], classes], axis=-1)\n\n        boxes_dict = dict()\n        for cls in range(n_classes):\n            mask = tf.equal(boxes[:, 5], cls)\n            mask_shape = mask.get_shape()\n            if mask_shape.ndims != 0:\n                class_boxes = tf.boolean_mask(boxes, mask)\n                boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes,\n                                                              [4, 1, -1],\n                                                              axis=-1)\n                boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1])\n                indices = tf.image.non_max_suppression(boxes_coords,\n                                                       boxes_conf_scores,\n                                                       max_output_size,\n                                                       iou_threshold)\n                class_boxes = tf.gather(class_boxes, indices)\n                boxes_dict[cls] = class_boxes[:, :5]\n\n        boxes_dicts.append(boxes_dict)\n\n    return boxes_dicts","metadata":{"trusted":true,"_uuid":"a32f6e7a74477dc1fe77b99ddcf72a776fea3a92"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**yolo_v3_model_class**\n\nDefines the full YOLOv3 model class (Yolo_v3) integrating all building blocks into a complete object detection pipeline:\n\n__init__:\nInitializes the model configuration (e.g., number of classes, model input size, NMS settings, data format).\n\n__call__:\n\nPreprocesses input image (scaling and formatting).\n\nPasses image through Darknet-53 for feature extraction.\n\nApplies three YOLO detection heads at different scales using yolo_convolution_block and yolo_layer.\n\nUses upsample and feature concatenation to fuse lower-level and higher-level features.\n\nCombines all scale predictions (detect1, detect2, detect3).\n\nConverts raw outputs into actual bounding boxes via build_boxes.\n\nApplies non_max_suppression to remove duplicates and low-confidence detections.\n\nReturns:\nA list of dictionaries for each image in the batch, mapping each detected class to its bounding boxes.","metadata":{}},{"cell_type":"code","source":"class Yolo_v3:\n    \"\"\"Yolo v3 model class.\"\"\"\n\n    def __init__(self, n_classes, model_size, max_output_size, iou_threshold,\n                 confidence_threshold, data_format=None):\n        \"\"\"Creates the model.\n\n        Args:\n            n_classes: Number of class labels.\n            model_size: The input size of the model.\n            max_output_size: Max number of boxes to be selected for each class.\n            iou_threshold: Threshold for the IOU.\n            confidence_threshold: Threshold for the confidence score.\n            data_format: The input format.\n\n        Returns:\n            None.\n        \"\"\"\n        if not data_format:\n            if tf.test.is_built_with_cuda():\n                data_format = 'channels_first'\n            else:\n                data_format = 'channels_last'\n\n        self.n_classes = n_classes\n        self.model_size = model_size\n        self.max_output_size = max_output_size\n        self.iou_threshold = iou_threshold\n        self.confidence_threshold = confidence_threshold\n        self.data_format = data_format\n\n    def __call__(self, inputs, training):\n        \"\"\"Add operations to detect boxes for a batch of input images.\n\n        Args:\n            inputs: A Tensor representing a batch of input images.\n            training: A boolean, whether to use in training or inference mode.\n\n        Returns:\n            A list containing class-to-boxes dictionaries\n                for each sample in the batch.\n        \"\"\"\n        with tf.variable_scope('yolo_v3_model'):\n            if self.data_format == 'channels_first':\n                inputs = tf.transpose(inputs, [0, 3, 1, 2])\n\n            inputs = inputs / 255\n\n            route1, route2, inputs = darknet53(inputs, training=training,\n                                               data_format=self.data_format)\n\n            route, inputs = yolo_convolution_block(\n                inputs, filters=512, training=training,\n                data_format=self.data_format)\n            detect1 = yolo_layer(inputs, n_classes=self.n_classes,\n                                 anchors=_ANCHORS[6:9],\n                                 img_size=self.model_size,\n                                 data_format=self.data_format)\n\n            inputs = conv2d_fixed_padding(route, filters=256, kernel_size=1,\n                                          data_format=self.data_format)\n            inputs = batch_norm(inputs, training=training,\n                                data_format=self.data_format)\n            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n            upsample_size = route2.get_shape().as_list()\n            inputs = upsample(inputs, out_shape=upsample_size,\n                              data_format=self.data_format)\n            axis = 1 if self.data_format == 'channels_first' else 3\n            inputs = tf.concat([inputs, route2], axis=axis)\n            route, inputs = yolo_convolution_block(\n                inputs, filters=256, training=training,\n                data_format=self.data_format)\n            detect2 = yolo_layer(inputs, n_classes=self.n_classes,\n                                 anchors=_ANCHORS[3:6],\n                                 img_size=self.model_size,\n                                 data_format=self.data_format)\n\n            inputs = conv2d_fixed_padding(route, filters=128, kernel_size=1,\n                                          data_format=self.data_format)\n            inputs = batch_norm(inputs, training=training,\n                                data_format=self.data_format)\n            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n            upsample_size = route1.get_shape().as_list()\n            inputs = upsample(inputs, out_shape=upsample_size,\n                              data_format=self.data_format)\n            inputs = tf.concat([inputs, route1], axis=axis)\n            route, inputs = yolo_convolution_block(\n                inputs, filters=128, training=training,\n                data_format=self.data_format)\n            detect3 = yolo_layer(inputs, n_classes=self.n_classes,\n                                 anchors=_ANCHORS[0:3],\n                                 img_size=self.model_size,\n                                 data_format=self.data_format)\n\n            inputs = tf.concat([detect1, detect2, detect3], axis=1)\n\n            inputs = build_boxes(inputs)\n\n            boxes_dicts = non_max_suppression(\n                inputs, n_classes=self.n_classes,\n                max_output_size=self.max_output_size,\n                iou_threshold=self.iou_threshold,\n                confidence_threshold=self.confidence_threshold)\n\n            return boxes_dicts","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**image_io_and_visualization**\n\nProvides helper functions for image preprocessing, class label loading, and drawing YOLOv3 detection results:\n\nload_images(img_names, model_size):\nLoads and resizes images to the model’s input size, returning a 4D NumPy array ready for inference.\n\nload_class_names(file_name):\nReads class labels from a text file, returning a list of class names.\n\ndraw_boxes(img_names, boxes_dicts, class_names, model_size):\nDraws predicted bounding boxes on each image:\n\nScales box coordinates back to original image size.\n\nDisplays class names and confidence scores.\n\nUses color-coding and font rendering for clarity.\n\nDisplays the resulting image with overlaid detections.\n\n","metadata":{}},{"cell_type":"code","source":"def load_images(img_names, model_size):\n    \"\"\"Loads images in a 4D array.\n\n    Args:\n        img_names: A list of images names.\n        model_size: The input size of the model.\n        data_format: A format for the array returned\n            ('channels_first' or 'channels_last').\n\n    Returns:\n        A 4D NumPy array.\n    \"\"\"\n    imgs = []\n\n    for img_name in img_names:\n        img = Image.open(img_name)\n        img = img.resize(size=model_size)\n        img = np.array(img, dtype=np.float32)\n        img = np.expand_dims(img, axis=0)\n        imgs.append(img)\n\n    imgs = np.concatenate(imgs)\n\n    return imgs\n\n\ndef load_class_names(file_name):\n    \"\"\"Returns a list of class names read from `file_name`.\"\"\"\n    with open(file_name, 'r') as f:\n        class_names = f.read().splitlines()\n    return class_names\n\n\ndef draw_boxes(img_names, boxes_dicts, class_names, model_size):\n    \"\"\"Draws detected boxes.\n\n    Args:\n        img_names: A list of input images names.\n        boxes_dict: A class-to-boxes dictionary.\n        class_names: A class names list.\n        model_size: The input size of the model.\n\n    Returns:\n        None.\n    \"\"\"\n    colors = ((np.array(color_palette(\"hls\", 80)) * 255)).astype(np.uint8)\n    for num, img_name, boxes_dict in zip(range(len(img_names)), img_names,\n                                         boxes_dicts):\n        img = Image.open(img_name)\n        draw = ImageDraw.Draw(img)\n        font = ImageFont.truetype(font='../input/futur.ttf',\n                                  size=(img.size[0] + img.size[1]) // 100)\n        resize_factor = \\\n            (img.size[0] / model_size[0], img.size[1] / model_size[1])\n        for cls in range(len(class_names)):\n            boxes = boxes_dict[cls]\n            if np.size(boxes) != 0:\n                color = colors[cls]\n                for box in boxes:\n                    xy, confidence = box[:4], box[4]\n                    xy = [xy[i] * resize_factor[i % 2] for i in range(4)]\n                    x0, y0 = xy[0], xy[1]\n                    thickness = (img.size[0] + img.size[1]) // 200\n                    for t in np.linspace(0, 1, thickness):\n                        xy[0], xy[1] = xy[0] + t, xy[1] + t\n                        xy[2], xy[3] = xy[2] - t, xy[3] - t\n                        draw.rectangle(xy, outline=tuple(color))\n                    text = '{} {:.1f}%'.format(class_names[cls],\n                                               confidence * 100)\n                    text_size = draw.textsize(text, font=font)\n                    draw.rectangle(\n                        [x0, y0 - text_size[1], x0 + text_size[0], y0],\n                        fill=tuple(color))\n                    draw.text((x0, y0 - text_size[1]), text, fill='black',\n                              font=font)\n\n        display(img)","metadata":{"trusted":true,"_uuid":"b992d6d649fdcb4c279deee99b2f66362066646c","_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**load_darknet_weights**\n\nLoads pretrained YOLOv3 weights (from the original Darknet .weights format) into the TensorFlow model:\n\nload_weights(variables, file_name):\n\nReads binary weights from file, skipping the first 5 metadata integers.\n\nDarknet Backbone (first 52 layers):\n\nLoads weights for convolution layers with batch normalization (no biases).\n\nYOLO Detection Heads:\n\nLoads weights for convolution layers, including three unnormalized layers (with biases).\n\nUses tf.assign() to map weight arrays to corresponding TensorFlow variables in the model.\n\nReturns:\nA list of TensorFlow assign operations that, when run in a session, apply the loaded weights to the model.","metadata":{}},{"cell_type":"code","source":"def load_weights(variables, file_name):\n    \"\"\"Reshapes and loads official pretrained Yolo weights.\n\n    Args:\n        variables: A list of tf.Variable to be assigned.\n        file_name: A name of a file containing weights.\n\n    Returns:\n        A list of assign operations.\n    \"\"\"\n    with open(file_name, \"rb\") as f:\n        # Skip first 5 values containing irrelevant info\n        np.fromfile(f, dtype=np.int32, count=5)\n        weights = np.fromfile(f, dtype=np.float32)\n\n        assign_ops = []\n        ptr = 0\n\n        # Load weights for Darknet part.\n        # Each convolution layer has batch normalization.\n        for i in range(52):\n            conv_var = variables[5 * i]\n            gamma, beta, mean, variance = variables[5 * i + 1:5 * i + 5]\n            batch_norm_vars = [beta, gamma, mean, variance]\n\n            for var in batch_norm_vars:\n                shape = var.shape.as_list()\n                num_params = np.prod(shape)\n                var_weights = weights[ptr:ptr + num_params].reshape(shape)\n                ptr += num_params\n                assign_ops.append(tf.assign(var, var_weights))\n\n            shape = conv_var.shape.as_list()\n            num_params = np.prod(shape)\n            var_weights = weights[ptr:ptr + num_params].reshape(\n                (shape[3], shape[2], shape[0], shape[1]))\n            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n            ptr += num_params\n            assign_ops.append(tf.assign(conv_var, var_weights))\n\n        # Loading weights for Yolo part.\n        # 7th, 15th and 23rd convolution layer has biases and no batch norm.\n        ranges = [range(0, 6), range(6, 13), range(13, 20)]\n        unnormalized = [6, 13, 20]\n        for j in range(3):\n            for i in ranges[j]:\n                current = 52 * 5 + 5 * i + j * 2\n                conv_var = variables[current]\n                gamma, beta, mean, variance =  \\\n                    variables[current + 1:current + 5]\n                batch_norm_vars = [beta, gamma, mean, variance]\n\n                for var in batch_norm_vars:\n                    shape = var.shape.as_list()\n                    num_params = np.prod(shape)\n                    var_weights = weights[ptr:ptr + num_params].reshape(shape)\n                    ptr += num_params\n                    assign_ops.append(tf.assign(var, var_weights))\n\n                shape = conv_var.shape.as_list()\n                num_params = np.prod(shape)\n                var_weights = weights[ptr:ptr + num_params].reshape(\n                    (shape[3], shape[2], shape[0], shape[1]))\n                var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n                ptr += num_params\n                assign_ops.append(tf.assign(conv_var, var_weights))\n\n            bias = variables[52 * 5 + unnormalized[j] * 5 + j * 2 + 1]\n            shape = bias.shape.as_list()\n            num_params = np.prod(shape)\n            var_weights = weights[ptr:ptr + num_params].reshape(shape)\n            ptr += num_params\n            assign_ops.append(tf.assign(bias, var_weights))\n\n            conv_var = variables[52 * 5 + unnormalized[j] * 5 + j * 2]\n            shape = conv_var.shape.as_list()\n            num_params = np.prod(shape)\n            var_weights = weights[ptr:ptr + num_params].reshape(\n                (shape[3], shape[2], shape[0], shape[1]))\n            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n            ptr += num_params\n            assign_ops.append(tf.assign(conv_var, var_weights))\n\n    return assign_ops","metadata":{"trusted":true,"_uuid":"24d4689103695a95a2634c4d688475ba2750fdea"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**preview_input_images**\n\nLoads and displays the raw input images (e.g., dog.jpg, office.jpg) before any preprocessing:\n\nHelps visually confirm that the input images exist and are being read correctly.\n\nUseful for inspection/debugging before running inference.\n\n","metadata":{}},{"cell_type":"code","source":"img_names = ['../input/dog.jpg', '../input/office.jpg']\nfor img in img_names: display(Image.open(img))","metadata":{"trusted":true,"_uuid":"a4db589483f06d03a5053efb80a624b54517925f","scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**run_yolo_inference**\n\nPerforms full YOLOv3 object detection on input images:\n\nPrepares:\n\nbatch of resized input images\n\nClass labels (coco.names)\n\nYOLOv3 model with thresholds and anchors\n\nLoads weights from official Darknet file (yolov3.weights)\n\nRuns inference using a TensorFlow session\n\nDraws bounding boxes and class labels on detected objects via draw_boxes\n\nPurpose: Executes model prediction pipeline end-to-end from input image to visual output.","metadata":{}},{"cell_type":"code","source":"batch_size = len(img_names)\nbatch = load_images(img_names, model_size=_MODEL_SIZE)\nclass_names = load_class_names('../input/coco.names')\nn_classes = len(class_names)\nmax_output_size = 10\niou_threshold = 0.5\nconfidence_threshold = 0.5\n\nmodel = Yolo_v3(n_classes=n_classes, model_size=_MODEL_SIZE,\n                max_output_size=max_output_size,\n                iou_threshold=iou_threshold,\n                confidence_threshold=confidence_threshold)\n\ninputs = tf.placeholder(tf.float32, [batch_size, 416, 416, 3])\n\ndetections = model(inputs, training=False)\n\nmodel_vars = tf.global_variables(scope='yolo_v3_model')\nassign_ops = load_weights(model_vars, '../input/yolov3.weights')\n\nwith tf.Session() as sess:\n    sess.run(assign_ops)\n    detection_result = sess.run(detections, feed_dict={inputs: batch})\n    \ndraw_boxes(img_names, detection_result, class_names, _MODEL_SIZE)","metadata":{"trusted":true,"_uuid":"40b4a8d68a58cc2c2fcccf0dc24345f92ae955c2"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**display_detection_gif**\n\nDisplays an animated GIF (e.g., detections.gif) showcasing the detection results:\n\nUses IPython.display.Image to render the binary image content.\n\nUseful for summarizing and presenting model performance visually across multiple frames or scenarios.\n\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nwith open('../input/detections.gif','rb') as f:\n    display(Image(data=f.read(), format='png'))","metadata":{"trusted":true,"_uuid":"fb46fa422366237bac8dfe281950aa44e7a9c798","_kg_hide-output":false,"_kg_hide-input":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Final Summary**\n\n\nThis project successfully implements a YOLOv3 object detection pipeline in TensorFlow 1.x, replicating the official Darknet version using:\n\nCustom Layer Implementations matching YOLOv3 specs.\n\nWeight Converter/Loader from original .weights file.\n\nDetection Visualization Tools for drawing bounding boxes and rendering animated GIFs.\n\nThis work enables:\n\nEasy transfer of pretrained YOLOv3 models into TensorFlow\n\nAdaptation for fine-tuning or custom dataset inference\n\nEducational insight into YOLOv3 internals and postprocessing","metadata":{}}]}